News Router

Introduction and Discussion

This application provides users with ability to define free-text queries that describe the contents of news articles they are interested in reading. The system fetches, stores, and indexes news articles based upon listings in approximately 50 RSS feeds from CNN, The New York Times, and BusinessWeek. It then scores the articles against existing queries, and routes articles to users who have queries that match the article. Articles are indexed using a dictionary and postings-list methodology. Each document is represented in the database by a vector of term-frequencies. At the time of scoring, raw term-frequecies are converted to tf-idf weights on-the-fly, and the query and document vectors are scored for similarity using the cosine similarity method. Only articles with scores above the the threshold score value for the query being scored against will be routed to the user as a match. It was found through some experimentation that a minimum threshold of 0.1 is required for good precision and recall. Anything below that yields very low recall levels or very poor precision, or both. Any threshold above 0.5 provides very good precision in most cases, but very low recall, as even very good articles are passed up.

Installation Instructions:

1.  This is a web application developed in Ruby on Rails and running on the Heroku hosting platform.  No installation is required. All Rails Model data is stored in PostgreSQL. All index-related data (dictionary and postings) are stored in MongoDB.
2.	Use the app at http://strong-mist-7332.herokuapp.com
3.	The full code can be accessed on the master branch of the github repo at https://github.com/brendte/news_router
4.	To use, visit http://strong-mist-7332.herokuapp.com and create an account. Once logged in, click My Queries, and create a free text query (the query "pope" with threshold 0.1 will work very well against articles indexed over the last week, for obvious reasons). Return to My Articles, and you may see some results, depending on the query and how long it is taking to run the query agains all known articles. Assuming the query matches some articles, you may have to refresh a couple of times before you start seeing some article on your list. Periodically throughout the days ahead, when you check back in your account, you will see any new articles that have been retrieved that match your query, if there are any.
5. I have included a couple of screenshots of my "My Articles" and "My Queries" pages. Since beyond this there really is no "running of the application" to screenshot, I'm going to leave it at this and assume that providing this document, the code and the code comments, and the ability to use the app in a live environment will be sufficient to meet this requirement. If you need to see more of what's going on behind the scenes (such as what the data in Postgres or Mongo looks like, please let me know and I can get you access).

Code Walkthrough

NOTE: The code is extensively commented, so this walkthrough provides only a high-level overview of the application, with references to code files for more details.

1.	The core non-index-related data for the application is represented by models, found in app/models, and stored in Heroku PostgreSQL as relational data. The models are named and commented in a way to make it easy to understand how they function and what they represent.
2.	The core IR functions of the system are contained within the classes in the app/classes directory.
    a. crawler.rb contains the Crawler class, which is responsible for reading and parsing RSS feeds (a list of which can be found in the db/seeds.rb file), and storing the feed entries in the feeds to the db as FeedEntry objects. In addition, it is responsible for calling out to AlchemyAPI with the article URL contained in each FeedEntry in order to retrieve the body text of each article pointed to by a FeedEntry. Each article is then stored as a new Article object in the db.
    b. indexer.rb contains the Indexer class, which is responsible for cleaning, stripping, tokenizing and stemming the Article body text, as well as building internal data structures to hold the words and frequencies before they are stored. It also does the heavy lifting of storing the term and df in the dictionary collection in MongoDB, as well as the document_id and tf for each term in a document in the postings collection of MongoDB.
    c. scorer.rb contains the Scorer class, which is responsible for calculating the cosine similarity between document vectors. It also contains methods for calculating the if-idf of a term and the euclidean length of documents. The cosine similarity algorithm used is based upon the FastCosineScore algorithm on p. 125 of "Introduction to Information Retrieval" by Manning, Raghavan, and Schutze, Cambridge University Press, 2009.
    c. article_router.rb contains the ArticleRouter class, which handles calling into Scorer with documents in order to get their similarity scores, and then determining if a document should be routed to a user based on the score and the user's query threshold for that query. It also handles actually storing a reference to an article in the user's articles collection when there is a match.
3. The cross-cutting concern of "being an indexable and scorable document" is represented by the Indexable module in concerns/indexable.rb. Any model that includes this module, and contains the proper field as noted in the Indexable comments, can be passed into Indexer and Scorer for indexing and scoring.
4. The "background" functionality of fetching new articles, indexing them, and then scoring and routing the articles to users is handled by a simple recurring background job scheduled and managed by the rufus_scheduler code in config/scheduler.rb. Every 30 minutes, refus_scheduler triggers this code, which makes the appropriate calls to Crawler, Indexer, and ArticleRouter to perform the actual work.
 5. When a user creates a new Query a girl_friday "job" is run on a new thread which scores all documents against this new query, and routes the matched documents to the user to whom the query belongs. This is run in a child thread in order to prevent this potentially long-running job from blocking the main Rails thread, which will cause the user to have to wait to do anything in his current web browser session until the query completes.

Issues

1. There are no known issues in this version. Doesn't mean there aren't any, it just means I haven't found them!

Ideas for Enhancement

1. I decided that having the originally planned Naive Bayes classifier would be too complicated for this project, so I left it out. The original intent was that this would be an additional means for routing articles to users, in that we could calculate the probability that a user will "like" a new article based on his/her classification of previous articles into "like" and "dislike" categories. Adding it back in would be a nice enhancement, I think.
2. Improve the running time of the Indexer methods. They aren't terribly slow, but could certainly be faster. Some ideas for this this are:
    a. Add more indexes to the MongoDB collections and PostgreSQL tables to improve query response times (although this could make Indexer run longer, because of the need to frequently update the DB indexes).
    b. Change the data structures used for the Index. MongoDB, while seemingly a good fit for this use case, may not be performant enough for really fast indexing on large collections.
3. Build a much more enhanced UI. I really just scaffolded this up quickly using Rails scaffolding and Zurb Foundation (http://foundation.zurb.com/) in order to have a minimally functional UI. It would be nice to have a single-page that presents all functionality (using Javascript and AJAX, among other things) to the user, rather than the current old-school request-response style. It would also be nice to be able to sort the articles, mark them as read, delete them from your list, and like/favorite them. This last feature would be necessary in some version in order to provide any recommendation or personalization that's based on machine learning.
4. Allow users to add their own RSS feeds, or choose to opt-out of system-provided feeds.
5. Add lots and lots of tests (as in, all of them, because there are none right now...)
